---
layout: default
title: Home
---

I am an assistant professor of [linguistics](https://linguistics.cornell.edu/) at [Cornell University](https://www.cornell.edu/). I am very interested in the incremental representations that humans use to process language, and in differences between how language is used and how it is processed. To explore these topics, I study the relationships between computational language models and psycholinguistic data (e.g., reading times) and I study neural network representations of language to understand what aspects of language can be learned from language statistics directly without having experiences in the real world (i.e. through ungrounded learning).

If you're interested in incremental processing models, you may find these helpful:  
* [LSTM toolkit that can estimate incremental processing difficulty](https://github.com/vansky/neural-complexity)  
* [125 pre-trained English LSTMs](https://zenodo.org/record/3559340)  
* [Left-corner parsing toolkit that can estimate incremental processing difficulty](https://github.com/modelblocks/modelblocks-release)

I manage the [Computational Psycholinguistics Discussions research group](/cpsyd.html) (C.Psyd) and am part of the [Cornell Computational Linguistics Lab](https://conf.ling.cornell.edu/compling/) (CLab) and the [Cornell Natural Language Processing Group](https://nlp.cornell.edu/) (Cornell NLP).

My surname is easy to pronounce (in words, not IPA): /van 'shine-dull/

### Recent News

Sept 18: 2 papers accepted at CoNLL.\
1) [Bhattacharya and van Schijndel (2020)](/assets/pdf/bhattacharya_vanschijndel-2020-conll.pdf): Neural networks encode abstract filler-gap existence but do not learn more abstract clusterings over kinds of filler-gaps. Raises questions about the depth of abstraction needed to process text sequences.\
2) [Davis and van Schijndel (2020)](/assets/pdf/davis_vanschijndel-2020-conll.pdf): Transformers encode implicit causality verb biases but fail to use that knowledge to make correct predictions. Validates Hartshorne's theory that IC is learnable from language sequences, but suggests that the language modeling objective prevents models from using this knowledge.

Aug 27: Submitted a paper showing that garden path effects cannot be predicted solely by surprisal. Feedback appreciated! [Single-stage prediction models do not explain the magnitude of syntactic disambiguation difficulty](https://psyarxiv.com/sgbqy/)

May 13: Gave an invited talk to CPL Lab at MIT: [Language is not Language Processing](/assets/pdf/vanschijndel-2020-invited_mit-slides.pdf)

April 30: Forrest Davis' [paper](/assets/pdf/davis_vanschijndel-2020-cogsci.pdf) was accepted to CogSci! Explores the ability to learn situation knowledge and discourse representations from plain text.

April 4: Forrest Davis' [paper](/assets/pdf/davis_vanschijndel-2020-acl.pdf) was accepted to ACL! Explores aspects of language comprehension that cannot be learned by current language models.

March 19: Forrest Davis' [CUNY presentation](https://osf.io/thj6c/) went well. Explores the ability to learn situation knowledge and discourse representations from plain text.
