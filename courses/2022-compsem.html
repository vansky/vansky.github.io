---
layout: page
title: Computational Seminar 2022 - Chaos Theory in Computational Linguistics
sidebar_link: false
order: 4
---

Neural networks compress corpus statistics down using intermediate layers. This compression process has been argued to strip away surface features and, in language processing, distill the statistics down to abstract linguistic groupings that exist underlyingly in language data. Thus linguistic abstractions can theoretically <i>emerge</i> in the network representation space simply due to the most efficient features along which to compress or cluster the observed statistics. In this class, we will read and discuss research that has examined these emergent properties and theorized about the ways emergent meaning can be structured.

<h3>Schedule</h3>

<h4>Week 1: Chaos Theory in CL</h4>
Syllabus<br>
<a href="/assets/lectures/2022-Seminar-Intro.pdf">Course Introduction</a><br>

<h4>Week 2: Linking Up and Digging In</h4>
<a href="https://psyarxiv.com/tbmcg/">Guest and Martin (2021). On logical inference over brains, behaviour, and artificial neural networks</a><br>
<a href="https://content.apa.org/doi/10.1037/0278-7393.30.2.431">Tabor and Hutchins (2004). Evidence for Self-Organized Sentence Processing: Digging-In Effects</a><br>

<br><br>
<br><br>
