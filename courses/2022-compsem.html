---
layout: page
title: Computational Seminar 2022 - Chaos Theory in Computational Linguistics
sidebar_link: false
order: 4
---

Neural networks compress corpus statistics down using intermediate layers. This compression process has been argued to strip away surface features and, in language processing, distill the statistics down to abstract linguistic groupings that exist underlyingly in language data. Thus linguistic abstractions can theoretically <i>emerge</i> in the network representation space simply due to the most efficient features along which to compress or cluster the observed statistics. In this class, we will read and discuss research that has examined these emergent properties and theorized about the ways emergent meaning can be structured.

<h3>Schedule</h3>

<h4>Week 1: Chaos Theory in CL</h4>
Syllabus<br>
<a href="/assets/lectures/2022-Seminar-Intro.pdf">Course Introduction</a><br>

<h4>Week 2: Linking Up and Digging In</h4>
<a href="https://psyarxiv.com/tbmcg/">Guest and Martin (2021). On logical inference over brains, behaviour, and artificial neural networks</a><br>
<a href="https://content.apa.org/doi/10.1037/0278-7393.30.2.431">Tabor and Hutchins (2004). Evidence for Self-Organized Sentence Processing: Digging-In Effects</a><br>

<h4>Week 3: Dynamical Modelling with Discrete States</h4>
<a href="https://psyarxiv.com/dtazq">Smith and Vasishth (2021). A software toolkit for modeling human sentence parsing: An approach using continuous-time, discrete-state stochastic dynamical systems</a><br>

<h4>Week 4: Cue-Based Retrieval</h4>
<a href="https://doi.org/10.1016/j.tics.2006.08.007">Lewis, Vasishth, and Van Dyke (2006). Computational principles of working memory in sentence comprehension</a><br>
<a href="https://doi.org/10.1016/j.tics.2021.07.002">Vasishth, Nicenboim, Engelmann, and Burchert (2019). Computational Models of Retrieval Processes in Sentence Processing</a><br>

<h4>Week 5: Gradient Symbolic Computation 1</h4>
<a href="https://www.degruyter.com/document/doi/10.1515/lingvan-2016-0105/pdf">Cho, Goldrick, and Smolensky (2017). Incremental parsing in a continuous dynamical system: sentence processing in Gradient Symbolic Computation</a><br>
<a href="https://arxiv.org/abs/1812.08718">McCoy, Linzen, Dunbar, and Smolensky (2019). RNNs Implicitly Implement Tensor Product Representations</a><br>

<h4>(Week 6: No class)</h4>

<h4>Week 7: Gradient Symbolic Computation 2</h4>
<a href="https://psyarxiv.com/utcgv/">Cho, Goldrick, and Smolensky (2022). Parallel parsing in a Gradient Symbolic Computation parser</a><br>

<h4>Week 8: Models of Memory</h4>
<a href="https://www.annualreviews.org/doi/10.1146/annurev-psych-010418-103358">Kahana (2020). Computational Models of Memory Search</a><br>

<h4>(Weeks 9-10: Group project planning/discussion)</h4>

<h4>(Week 11: Spring Break)</h4>

<h4>Week 12: Processing Errors in Case-Marked Languages</h4>
<a href="https://osf.io/ydetz/">Apurva and Husain (2021). Parsing errors in Hindi: Investigating limits to verbal prediction in an SOV language</a><br>

<h4>(Weeks 13-16: Group project analysis and discussion)</h4>

<br><br>
<br><br>
