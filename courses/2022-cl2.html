---
layout: page
title: Computational Linguistics 2 (Fall 2022)
sidebar_link: false
order: 4
---

While LING 4424: Computational Linguistics 1 focuses on symbolic computational linguistics methods (n-gram smoothing, hidden markov modeling, probabilistic context-free grammars, etc), CL2 provides an introduction to neural networks and the techniques we can use for inferring the linguistic knowledge they encode. This course is a work in progress, so any feedback/suggestions are appreciated.

<h3>Syllabus</h3>
<a href="/assets/pdf/cl2-syllabus-2022.pdf">pdf</a><br>

<h3>Tentative Schedule</h3>

<h4>Weeks 1-3: Background Lectures</h4>
Neural network basics/history<br>
PyTorch overview<br>
Neural network architectures<br>

<h4>Week 4: Behavioral analyses</h4>
Required<br>
<a href="https://www.aclweb.org/anthology/Q16-1037/">Linzen et al. (2016). Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies</a><br>
Optional<br>
<a href="https://www.aclweb.org/anthology/N18-1108/">Gulordava et al. (2018). Colorless Green Recurrent Networks Dream Hierarchically</a><br>
<a href="https://www.aclweb.org/anthology/W18-5423/">Wilcox et al. (2018). What do RNN Language Models Learn about Fillerâ€“Gap Dependencies?</a><br>
<a href="https://scholarworks.umass.edu/scil/vol3/iss1/4/">Chaves (2020). What Don't RNN Language Models Learn About Filler-Gap Dependencies?</a><br>
<a href="https://www.aclweb.org/anthology/2020.acl-main.479/">Schuster et al. (2020). Harnessing the linguistic signal to predict scalar inferences</a><br>

<h4>Week 5: Diagnostic classifiers</h4>
Required<br>
<a href="https://www.aclweb.org/anthology/W18-5426/">Giulianelli et al. (2018). Under the Hood</a><br>
Optional<br>
<a href="https://www.aclweb.org/anthology/D16-1079/">Qian et al. (2016). Analyzing Linguistic Knowledge in Sequential Model of Sentence</a><br>
<a href="https://arxiv.org/abs/1608.04207">Adi et al. (2016). Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks</a><br>
<a href="https://aclanthology.org/K19-1001/">Jumelet et al. (2019). Analysing Neural Language Models: Contextual Decomposition Reveals Default Reasoning in Number and Gender Assignment.<a><br>

<h4>Week 6: Adaptation-as-priming</h4>
Required<br>
<a href="https://www.aclweb.org/anthology/K19-1007/">Prasad et al. (2019). Using Priming to Uncover the Organization of Syntactic Representations in Neural Language Models</a><br>
Optional<br>
<a href="https://www.aclweb.org/anthology/D18-1499/">van Schijndel and Linzen (2018). A Neural Model of Adaptation in Reading</a><br>
<a href="https://www.aclweb.org/anthology/2020.acl-main.303/">Lepori et al. (2020). Representations of Syntax [MASK] Useful: Effects of Constituency and Dependency Structure in Recursive LSTMs.</a><br>
<a href="https://aclanthology.org/2020.conll-1.39/">Debasmita and van Schijndel (2020). Filler-gaps that neural networks fail to generalize.</a><br>

<h4>Weeks 7-8: Probe validation</h4>
Required (Week 7)<br>
<a href="https://www.aclweb.org/anthology/P19-1334.pdf">McCoy et al. (2019). Right for the Wrong Reasons</a><br>
Required (Week 8)<br>
<a href="https://arxiv.org/abs/2003.12298">Voita and Titov. (2020). Information-Theoretic Probing with Minimum Description Length</a><br>
Optional<br>
<a href="https://www.aclweb.org/anthology/D19-1275/">Hewitt and Liang. (2019). Designing and Interpreting Probes with Control Tasks</a><br>
<a href="https://www.aclweb.org/anthology/2020.acl-main.420/">Pimentel et al. (2020). Information-Theoretic Probing for Linguistic Structure</a><br>

<h4>Weeks 9-10: Group projects</h4>
Tuesdays: Project outlines/discussion<br>
Thursdays: Group-suggested paper discussion<br><br>
    
<br><br>
<br><br>
