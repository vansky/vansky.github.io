---
layout: page
title: Computational Linguistics 2
sidebar_link: false
order: 4
---

I teach LING 4434: Computational Linguistics 2. While LING 4424: Computational Linguistics 1 focuses on symbolic computational linguistics methods (n-gram smoothing, hidden markov modeling, probabilistic context-free grammars, etc), CL2 focuses on neural networks. Specifically, it focuses on techniques for inferring the linguistic knowledge encoded in neural network language models. This course is a work in progress, so any feedback/suggestions are appreciated.

<h3>Syllabus</h3>
<a href="/assets/pdf/cl2-syllabus-2020.pdf">pdf</a><br>

<h3>Schedule</h3>

<h4>Weeks 1-3: Background Lectures</h4>
Neural network basics/history<br>
PyTorch overview<br>
Neural network architectures<br>

<h4>Week 4: Behavioral analyses</h4>
Required<br>
<a href="https://www.aclweb.org/anthology/Q16-1037/">Linzen et al. (2016). Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies</a><br>
Optional<br>
<a href="https://www.aclweb.org/anthology/N18-1108/">Gulordava et al. (2018). Colorless Green Recurrent Networks Dream Hierarchically</a><br>
<a href="https://www.aclweb.org/anthology/W18-5423/">Wilcox et al. (2020). What do RNN Language Models Learn about Fillerâ€“Gap Dependencies?</a><br>
<a href="https://scholarworks.umass.edu/scil/vol3/iss1/4/">Chaves (2020). What Don't RNN Language Models Learn About Filler-Gap Dependencies?</a><br>
<a href="https://www.aclweb.org/anthology/2020.acl-main.479/">Schuster et al. (2020). Harnessing the linguistic signal to predict scalar inferences</a><br>

<h4>Week 5: Diagnostic classifiers</h4>
Required<br>
<a href="https://www.aclweb.org/anthology/W18-5426/">Giulianelli et al. (2018). Under the Hood</a><br>
Optional<br>
<a href="https://www.aclweb.org/anthology/D16-1079/">Qian et al. (2016). Analyzing Linguistic Knowledge in Sequential Model of Sentence</a><br>
<a href="https://arxiv.org/abs/1608.04207">Adi et al. (2016). Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks</a><br>

<h4>Week 6: Adaptation-as-priming</h4>
Required<br>
<a href="https://www.aclweb.org/anthology/K19-1007/">Prasad et al. (2019). Using Priming to Uncover the Organization of Syntactic Representations in Neural Language Models</a><br>
Optional<br>
<a href="https://www.aclweb.org/anthology/D18-1499/">van Schijndel and Linzen (2018). A Neural Model of Adaptation in Reading</a><br>
<a href="https://www.aclweb.org/anthology/2020.acl-main.303/">Lepori et al. (2020). Representations of Syntax [MASK] Useful: Effects of Constituency and Dependency Structure in Recursive LSTMs.</a><br>

<h4>Weeks 7-8: Probe validation</h4>
Required (Week 7)<br>
<a href="https://www.aclweb.org/anthology/P19-1334.pdf">McCoy et al. (2019). Right for the Wrong Reasons</a><br>
Required (Week 8)<br>
<a href="https://arxiv.org/abs/2003.12298">Voita and Titov. (2020). Information-Theoretic Probing with Minimum Description Length</a><br>
Optional<br>
<a href="https://www.aclweb.org/anthology/D19-1275/">Hewitt and Liang. (2019). Designing and Interpreting Probes with Control Tasks</a><br>
<a href="https://www.aclweb.org/anthology/2020.acl-main.420/">Pimentel et al. (2020). Information-Theoretic Probing for Linguistic Structure</a><br>

<h4>Weeks 9-10: Group projects</h4>
Tuesdays: Project outlines/discussion<br>
Thursdays: Group-suggested paper discussion<br><br>

Group 1: NMT attention probing<br>
Tuesday<br>
Attention Tutorial<br>
Reading: <a href="https://www.aclweb.org/anthology/D19-1002.pdf">Wiegreffe and Pinter (2019). Attention is Not Not Explanation</a><br>
Optional: <a href="https://www.aclweb.org/anthology/N19-1357.pdf">Jain and Wallace (2019) Attention is Not Explanation</a><br><br>

Thursday<br>
Reading: <a href="https://medium.com/@byron.wallace/thoughts-on-attention-is-not-not-explanation-b7799c4c3b24">Response post by Byron Wallace</a><br>
Project discussion<br><br>

Group 2: Morphology in word-level LMs<br>
Tuesday<br>
Reading: <a href="https://www.aclweb.org/anthology/D19-1002.pdf">Xu et al. (2018). Incorporating Latent Meanings of Morphological Compositions to Enhance Word Embeddings</a><br><br>

Thursday<br>
Paper overview: <a href="https://www.aclweb.org/anthology/N18-1108/">Gulordava et al. (2018). Colorless Green Recurrent Networks Dream Hierarchically</a><br>
Project discussion<br><br>

<h4>Week 11: Misc probing</h4>
Tuesday Reading (RSA): <a href="https://www.aclweb.org/anthology/P19-1283.pdf">Chrupala and Alishahi (2019). Correlating neural and symbolic representations of language</a><br>
Thursday Reading (ablation; optional): <a href="https://www.aclweb.org/anthology/N19-1002/">Lakretz et al. (2019). The emergence of number and syntax units in LSTM language models</a><br>

<h4>Weeks 12-13: Semi-finals (No class)</h4>
There are two CL conferences happening (virtually) during this period.<br>
Registering for one gets the other free. <a href="https://2020.emnlp.org/registration/">Register here.</a><br>
Cost: $100 (50 for EMNLP/CoNLL and 50 to become an ACL member)<br>
Register by Oct 30.<br>
<a href="https://2020.emnlp.org/">The 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a><br>
<a href="https://conll.org/">The SIGNLL Conference on Computational Natural Language Learning (CoNLL)</a><br>

<h4>Week 14: Poverty of the Stimulus</h4>
<a href="https://www.aclweb.org/anthology/2020.acl-main.463/">Bender and Koller. (2020). Climbing towards NLU</a><br>
<a href="https://www.aclweb.org/anthology/2020.acl-main.179/">Davis & van Schijndel (2020). Recurrent Neural Network Language Models Always Learn English-Like Relative Clause Attachment</a><br>
<a href="https://arxiv.org/abs/2004.10151">Bisk et al. (unpublished). Experience grounds language</a><br>

<h4>Week 15</h4>
Topic TBD<br>

<h4>Week 16</h4>
Project 2 presentations<br>

<br><br>
<br><br>
