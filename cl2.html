---
layout: page
title: Computational Linguistics 2
sidebar_link: false
order: 4
---

I teach LING 4434: Computational Linguistics 2. While LING 4424: Computational Linguistics 1 focuses on symbolic computational linguistics methods (n-gram smoothing, hidden markov modeling, probabilistic context-free grammars, etc), CL2 focuses on neural networks. Specifically, it focuses on techniques for inferring the linguistic knowledge encoded in neural network language models. This course is a work in progress, so any feedback/suggestions are appreciated.

<h3>Syllabus</h3>
<a href="/assets/pdf/cl2-syllabus-2020.pdf">pdf</a><br>

<h3>Schedule</h3>

<h4>Weeks 1-3</h4>
Neural network basics/history<br>
PyTorch overview<br>
Neural network architectures<br>

<h4>Week 4</h4>
Behavioral analyses<br>
<a href="https://www.aclweb.org/anthology/Q16-1037/">Linzen et al. (2016). Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies</a><br>
<a href="https://www.aclweb.org/anthology/N18-1108/">Gulordava et al. (2018). Colorless Green Recurrent Networks Dream Hierarchically</a><br>
<a href="https://www.aclweb.org/anthology/2020.acl-main.179/">Davis & van Schijndel (2020). Recurrent Neural Network Language Models Always Learn English-Like Relative Clause Attachment</a><br>
<a href="https://www.aclweb.org/anthology/W18-5423/">Wilcox et al. (2020). What do RNN Language Models Learn about Fillerâ€“Gap Dependencies?</a><br>
<a href="https://scholarworks.umass.edu/scil/vol3/iss1/4/">Chaves (2020). What Don't RNN Language Models Learn About Filler-Gap Dependencies?</a><br>
<a href="https://www.aclweb.org/anthology/2020.acl-main.479/">Schuster et al. (2020). Harnessing the linguistic signal to predict scalar inferences</a><br>

<h4>Week 5</h4>
Diagnostic classifiers<br>
<a href="https://www.aclweb.org/anthology/D16-1079/">Qian et al. (2016). Analyzing Linguistic Knowledge in Sequential Model of Sentence</a><br>
<a href="https://arxiv.org/abs/1608.04207">Adi et al. (2016). Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks</a><br>

<h4>Week 6</h4>
Priming tasks<br>
<a href="https://www.aclweb.org/anthology/K19-1007/">Prasad et al. (2019). Using Priming to Uncover the Organization of Syntactic Representations in Neural Language Models</a><br>

<h4>Weeks 7-8</h4>
Probe validation<br>
<a href="https://www.aclweb.org/anthology/P19-1334.pdf">McCoy et al. (2019). Right for the Wrong Reasons</a><br>
<a href="https://www.aclweb.org/anthology/D19-1275/">Hewitt and Liang. (2019). Designing and Interpreting Probes with Control Tasks</a><br>
<a href="https://www.aclweb.org/anthology/2020.acl-main.420/">Pimentel et al. (2020). Information-Theoretic Probing for Linguistic Structure</a><br>
<a href="https://arxiv.org/abs/2003.12298">Voita and Titov. (unpublished). Information-Theoretic Probing with Minimum Description Length.</a><br>

<h4>Weeks 9-12</h4>
Project 1 presentations/discussion<br>
Project 2 planning<br>

<h4>Week 13</h4>
Intervention Probing<br>
<a href="https://www.aclweb.org/anthology/W18-5426/">Giulianelli et al. (2018). Under the Hood</a><br>
<a href="https://www.aclweb.org/anthology/N19-1002/">Lakretz et al. (2019). The emergence of number and syntax units in LSTM language models.</a><br>

<h4>Week 14</h4>
Poverty of the Stimulus<br>
<a href="https://www.aclweb.org/anthology/2020.acl-main.463/">Bender and Koller. (2020). Climbing towards NLU</a><br>
<a href="https://arxiv.org/abs/2004.10151">Bisk et al. (2020). Experience grounds language.</a><br>

<h4>Week 15</h4>
Topic TBD<br>

<h4>Week 16</h4>
Project 2 presentations<br>

<br><br>
<br><br>
